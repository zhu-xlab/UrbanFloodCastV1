{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "318b41ba-15a6-4e80-b2ed-edcc70baea6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "!export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b59bfb-9ca5-41c5-81f0-d712edb8bed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from systems_pbc import *\n",
    "import torch.backends.cudnn as cudnn\n",
    "from utils import *\n",
    "from visualize import *\n",
    "import matplotlib.pyplot as plt\n",
    "from models import FNN3d\n",
    "from train_utils import Adam\n",
    "from tqdm import tqdm\n",
    "from train_utils.losses import GeoPC_loss\n",
    "import matplotlib.image as pm\n",
    "import torch.nn as nn\n",
    "# from mpl_toolkits.basemap import Basemap\n",
    "import tifffile\n",
    "from data_ import utils\n",
    "import scipy.ndimage\n",
    "import boundary\n",
    "import boundary_gt\n",
    "from PIL import Image\n",
    "from pyMesh import visualize2D, setAxisLabel\n",
    "import train_utils.tensorboard as tb\n",
    "from AWL import AutomaticWeightedLoss\n",
    "import torch.nn.functional as F\n",
    "from hydraulics import saint_venant\n",
    "from scipy import interpolate\n",
    "from matplotlib.colors import ListedColormap\n",
    "# from skimage.transform import resize\n",
    "from train_utils.losses import *\n",
    "from torch.utils.data import DataLoader\n",
    "from pyMesh import visualize2D\n",
    "import imageio\n",
    "import io\n",
    "import os\n",
    "import tifffile as tiff\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "gpu_ids = [0]\n",
    "output_device = gpu_ids[0]\n",
    "\n",
    "\n",
    "\n",
    "################\n",
    "# Arguments\n",
    "################\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        # Basic configuration\n",
    "        self.loss_style = 'mean'  # Loss for the network (MSE vs. summing)\n",
    "        self.visualize = True  # Visualize the solution\n",
    "        self.save_model = True  # Save the model for analysis later\n",
    "\n",
    "        # PINO_model configuration\n",
    "        self.layers = [16, 24, 24, 32, 32]  # Dimensions/layers of the NN\n",
    "        self.modes1 = [32, 32, 32, 32]  # Modes for first dimension\n",
    "        self.modes2 = [32, 32, 32, 32]  # Modes for second dimension\n",
    "        self.modes3 = [8, 8, 8, 8]  # Modes for third dimension\n",
    "        self.fc_dim = 128  # Fully connected layer dimension\n",
    "        self.epochs = 15000  # Number of epochs\n",
    "        self.activation = 'gelu'  # Activation function to use in the network\n",
    "\n",
    "        # Training configuration\n",
    "        self.base_lr = 0.001  # Learning rate\n",
    "        self.milestones = [500, 1000, 2000, 3000, 4000, 5000]  # Milestones for learning rate scheduler\n",
    "        self.scheduler_gamma = 0.5  # Gamma for learning rate scheduler\n",
    "        self.theta = 0.7  # q-centered weighting. [0,1]\n",
    "\n",
    "    def __str__(self):\n",
    "        return (f'Args(loss_style={self.loss_style}, visualize={self.visualize}, save_model={self.save_model}, '\n",
    "                f'layers={self.layers}, modes1={self.modes1}, modes2={self.modes2}, modes3={self.modes3}, '\n",
    "                f'fc_dim={self.fc_dim}, epochs={self.epochs}, activation={self.activation}, '\n",
    "                f'base_lr={self.base_lr}, milestones={self.milestones}, scheduler_gamma={self.scheduler_gamma}, '\n",
    "                f'theta={self.theta})')\n",
    "\n",
    "# Example of how to use the Args class:\n",
    "args = Args()\n",
    "\n",
    "# Accessing an attribute\n",
    "print(\"Layers:\", args.layers)\n",
    "\n",
    "# Print all arguments\n",
    "print(args)\n",
    "\n",
    "import sys\n",
    "\n",
    "# CUDA support\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(device)\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(device)\n",
    "device = torch.device('cpu')\n",
    "output_device=device\n",
    "print(device)\n",
    "############################\n",
    "# Process data\n",
    "###########################\n",
    "def inter(array, size):\n",
    "    h, w = array.shape\n",
    "    new_h, new_w = np.floor_divide((h, w), size)\n",
    "    x = np.linspace(0, w - 1, w)\n",
    "    y = np.linspace(0, h - 1, h)\n",
    "    new_x = np.linspace(0, w - 1, new_w)\n",
    "    new_y = np.linspace(0, h - 1, new_h)\n",
    "    f = interpolate.interp2d(x, y, array, kind='linear')\n",
    "    array_down = f(new_x, new_y)\n",
    "    # array_down = resize(array, (new_h, new_w), order=1, anti_aliasing=True)\n",
    "    return array_down\n",
    "\n",
    "# Parameters\n",
    "g = torch.tensor(9.80616, dtype=torch.float64)\n",
    "dem_tif_path = '/home/sirui/INNOMAUS/output/115mm/r2d_bln1_swmm_b_115mm_0H.tif'#'/mnt/SSD2/qingsong/qinqsong/data_Berlin2/input/moa_bottom.tif'\n",
    "input_path = '/home/sirui/INNOMAUS/output'#'/mnt/SSD2/qingsong/qinqsong/data_Berlin2/Moabit/Moa_Randm/val'\n",
    "imput_path_val = '/home/sirui/INNOMAUS/val'#'/mnt/SSD2/qingsong/qinqsong/data_Berlin2/Moabit/Moa_Randm/val'\n",
    "man_path = '/home/sirui/INNOMAUS/output/115mm/r2d_bln1_swmm_b_115mm_0H.tif'#'/mnt/SSD2/qingsong/qinqsong/data_Berlin2/input/moa_rough.tif'\n",
    "dem_map = tifffile.imread(dem_tif_path)\n",
    "dem_map = inter(dem_map, 8)\n",
    "print('dem_map', dem_map.shape)\n",
    "TILE_SIZE_X = 2000\n",
    "TILE_SIZE_Y = 2000\n",
    "ALLOWED_MASKED_PERCENTAGE = 0\n",
    "MAX_TOPOGRAPHY_DIFFERENCE = 2000\n",
    "\n",
    "# DEM z\n",
    "def process_dem(dem_map):\n",
    "    np_ma_map = np.ma.masked_array(dem_map, mask=(dem_map < -2000))\n",
    "    np_ma_map = utils.fix_missing_values(np_ma_map)\n",
    "    dem = torch.from_numpy(np_ma_map)\n",
    "    return dem.float()\n",
    "# Precipitation\n",
    "\n",
    "def cfl(dx: float, max_h: torch.Tensor, alpha: float) -> torch.Tensor:\n",
    "    return alpha * dx / (g + max_h)\n",
    "\n",
    "\n",
    "def data(path, name, files):\n",
    "    t0 = 25\n",
    "    dt0 = 300\n",
    "    t00, tfinal = 0, (t0) * dt0\n",
    "    dx = 30.0 * 16\n",
    "    # # data\n",
    "    h_gt = []\n",
    "    qx_gt = []\n",
    "    qy_gt = []\n",
    "    for i in range(t00,tfinal,dt0):    \n",
    "        current_time = str(i)\n",
    "        print('current_time', current_time)\n",
    "        path_h = os.path.join(path,'r2d_bln1_swmm_b_'+ '%s_%s'%(name,current_time)+'H'+\".tif\")\n",
    "        path_u = os.path.join(path, 'r2d_bln1_swmm_b_'+ '%s_%s'%(name,current_time)+'U'+\".tif\")\n",
    "        path_v = os.path.join(path, 'r2d_bln1_swmm_b_'+ '%s_%s'%(name,current_time)+'V'+\".tif\")\n",
    "        # path_h = path_h.replace(\"'\", \"\\\"\")\n",
    "        # print('path_h', path_h)\n",
    "        h_current = tiff.imread(path_h)\n",
    "        h_current = np.array(h_current)\n",
    "        h_current = np.nan_to_num(h_current, nan=0.0)\n",
    "        h_current = inter(h_current, 8)\n",
    "        print('h_current', np.max(h_current))\n",
    "        h_current = torch.from_numpy(h_current)\n",
    "        h_current = h_current.float()\n",
    "        qx_current = tiff.imread(path_u)\n",
    "        qx_current = np.array(qx_current)\n",
    "        qx_current = np.nan_to_num(qx_current, nan=0.0)\n",
    "        qx_current = inter(qx_current, 8)\n",
    "        print('qx_current', np.max(qx_current))\n",
    "        qx_current = torch.from_numpy(qx_current)\n",
    "        qx_current = qx_current.float()\n",
    "        qy_current = tiff.imread(path_v)\n",
    "        qy_current = np.array(qy_current)\n",
    "        qy_current = np.nan_to_num(qy_current, nan=0.0)\n",
    "        qy_current = inter(qy_current, 8)\n",
    "        print('qy_current', np.max(qy_current))\n",
    "        qy_current = torch.from_numpy(qy_current)\n",
    "        qy_current = qy_current.float()\n",
    "        h_gt.append(h_current)\n",
    "        qx_gt.append(qx_current)\n",
    "        qy_gt.append(qy_current)\n",
    "    h_gt = torch.stack(h_gt, 0)\n",
    "    u_gt = torch.stack(qx_gt, 0)\n",
    "    v_gt = torch.stack(qy_gt, 0)\n",
    "    print('len_supervised', h_gt.size())\n",
    "    # pre\n",
    "    pre = np.zeros((t0))\n",
    "    for root, _, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(\".txt\"):\n",
    "                pre_path = os.path.join(root, file)\n",
    "                print('pre_path',pre_path)\n",
    "                with open(pre_path, 'r') as fil:\n",
    "                    for line in fil:\n",
    "                        condition, value = line.strip().split()\n",
    "                        m = int(int(condition)/dt0)\n",
    "                        if m <= t0:\n",
    "                            pre[m] = value\n",
    "    print('pre',pre)\n",
    "    return h_gt, u_gt, v_gt, pre\n",
    "\n",
    "\n",
    "class train_data():\n",
    "    def __init__(self, train=True):\n",
    "        super().__init__()\n",
    "        # self.data_res = data_res\n",
    "        # self.pde_res = pde_res\n",
    "        # self.t_duration = t_duration\n",
    "        # self.paths = paths\n",
    "        # self.offset = offset\n",
    "        # self.n_samples = n_samples\n",
    "        self.load(train=train)\n",
    "\n",
    "    def load(self, train=True):\n",
    "        if train:\n",
    "            t0 = 25\n",
    "            # days_train = 12\n",
    "            # T = 86400\n",
    "            lmbdleft, lmbdright = 0, (dem_map.shape[0] - 1)\n",
    "            thtlower, thtupper = 0, (dem_map.shape[1] - 1)\n",
    "            dt = 300\n",
    "            t00, tfinal = 0, (t0 - 1)\n",
    "            m = dem_map.shape[0]\n",
    "            n = dem_map.shape[1]\n",
    "            t = np.linspace(t00, tfinal, t0)\n",
    "            x = np.linspace(lmbdleft, lmbdright, m)\n",
    "            y = np.linspace(thtlower, thtupper, n)\n",
    "            data_star = np.hstack((x.flatten(), y.flatten(), t.flatten()))\n",
    "            lb = data_star.min(0)\n",
    "            ub = data_star.max(0)\n",
    "            input_data_list = []\n",
    "            h_gt_list = []\n",
    "            u_gt_list = []\n",
    "            v_gt_list = []\n",
    "            z_list = []\n",
    "            for root, directories, files in os.walk(input_path):\n",
    "                for subdirectory in directories:\n",
    "                  path = os.path.join(root, subdirectory)\n",
    "                  name = os.path.basename(path)\n",
    "                  if name and name[0].isdigit():###\n",
    "                    print(path)\n",
    "                    print('Name',name)\n",
    "                    h_gt, u_gt, v_gt, pre = data(path, name, files)\n",
    "                    gridx = torch.from_numpy(x)\n",
    "                    gridx = gridx.reshape(1, m, 1, 1, 1).repeat([1, 1, n, t0, 1])\n",
    "                    gridy = torch.from_numpy(y)\n",
    "                    gridy = gridy.reshape(1, 1, n, 1, 1).repeat([1, m, 1, t0, 1])\n",
    "                    gridt = torch.from_numpy(t)\n",
    "                    gridt = gridt.reshape(1, 1, 1, t0, 1).repeat([1, m, n, 1, 1])\n",
    "                    gridpre = torch.from_numpy(pre)\n",
    "                    gridpre = gridpre.reshape(1, 1, 1, t0, 1).repeat([1, m, n, 1, 1])\n",
    "                    #print(h_gt.shape)\n",
    "                    h_init = h_gt[0, :, :]\n",
    "                    h_init = h_init.reshape(1, m, n, 1, 1).repeat([1, 1, 1, t0, 1])\n",
    "                    input_data = torch.cat((gridx, gridy, gridt, gridpre), dim=-1)\n",
    "                    # input_data = 2.0 * (input_data - lb) / (ub - lb) - 1.0\n",
    "                    input_data = torch.cat((input_data, h_init.cpu()), dim=-1)\n",
    "                    input_data = input_data.float()\n",
    "                    h_gt = torch.unsqueeze(h_gt, dim=0)\n",
    "                    u_gt = torch.unsqueeze(u_gt, dim=0)\n",
    "                    v_gt = torch.unsqueeze(v_gt, dim=0)\n",
    "                    # h_init = gen_init(ini_height, ini_discharge, downsampling=True)\n",
    "                    # data_condition = [h_gt, qx_gt, qy_gt]\n",
    "                    # data_condition0 = data_condition\\\n",
    "                    z = process_dem(dem_map)\n",
    "                    z = torch.unsqueeze(z, dim=0)\n",
    "                    input_data_list.append(input_data)\n",
    "                    h_gt_list.append(h_gt)\n",
    "                    u_gt_list.append(u_gt)\n",
    "                    v_gt_list.append(v_gt)\n",
    "                    z_list.append(z)\n",
    "            data_input = torch.cat(input_data_list, dim=0)\n",
    "            gt_h = torch.cat(h_gt_list, dim=0)\n",
    "            gt_u = torch.cat(u_gt_list, dim=0)\n",
    "            gt_v = torch.cat(v_gt_list, dim=0)\n",
    "            data_z = torch.cat(z_list, dim=0)\n",
    "            self.data_input = data_input\n",
    "            self.gt_h = gt_h\n",
    "            self.gt_u = gt_u\n",
    "            self.gt_v = gt_v\n",
    "            self.data_z = data_z\n",
    "        else:\n",
    "            t0 = 25\n",
    "            # days_train = 12\n",
    "            # T = 86400\n",
    "            lmbdleft, lmbdright = 0, (dem_map.shape[0] - 1)\n",
    "            thtlower, thtupper = 0, (dem_map.shape[1] - 1)\n",
    "            dt = 300\n",
    "            t00, tfinal = 0, (t0 - 1)\n",
    "            m = dem_map.shape[0]\n",
    "            n = dem_map.shape[1]\n",
    "            t = np.linspace(t00, tfinal, t0)\n",
    "            x = np.linspace(lmbdleft, lmbdright, m)\n",
    "            y = np.linspace(thtlower, thtupper, n)\n",
    "            data_star = np.hstack((x.flatten(), y.flatten(), t.flatten()))\n",
    "            lb = data_star.min(0)\n",
    "            ub = data_star.max(0)\n",
    "            input_data_list = []\n",
    "            h_gt_list = []\n",
    "            u_gt_list = []\n",
    "            v_gt_list = []\n",
    "            z_list = []\n",
    "            for root, directories, files in os.walk(imput_path_val):\n",
    "                for subdirectory in directories:\n",
    "                  path = os.path.join(root, subdirectory)\n",
    "                  name = os.path.basename(path)\n",
    "                  if name and name[0].isdigit():###\n",
    "                    print(path)\n",
    "                    print('Name', name)\n",
    "                    \n",
    "                    h_gt, u_gt, v_gt, pre = data(path, name, files)\n",
    "                    gridx = torch.from_numpy(x)\n",
    "                    gridx = gridx.reshape(1, m, 1, 1, 1).repeat([1, 1, n, t0, 1])\n",
    "                    gridy = torch.from_numpy(y)\n",
    "                    gridy = gridy.reshape(1, 1, n, 1, 1).repeat([1, m, 1, t0, 1])\n",
    "                    gridt = torch.from_numpy(t)\n",
    "                    gridt = gridt.reshape(1, 1, 1, t0, 1).repeat([1, m, n, 1, 1])\n",
    "                    gridpre = torch.from_numpy(pre)\n",
    "                    gridpre = gridpre.reshape(1, 1, 1, t0, 1).repeat([1, m, n, 1, 1])\n",
    "                    #print(h_gt.shape,\"val\")\n",
    "                    h_init = h_gt[0, :, :]\n",
    "                    h_init = h_init.reshape(1, m, n, 1, 1).repeat([1, 1, 1, t0, 1])\n",
    "                    input_data = torch.cat((gridx, gridy, gridt, gridpre), dim=-1)\n",
    "                    # input_data = 2.0 * (input_data - lb) / (ub - lb) - 1.0\n",
    "                    input_data = torch.cat((input_data, h_init.cpu()), dim=-1)\n",
    "                    input_data = input_data.float()\n",
    "                    h_gt = torch.unsqueeze(h_gt, dim=0)\n",
    "                    u_gt = torch.unsqueeze(u_gt, dim=0)\n",
    "                    v_gt = torch.unsqueeze(v_gt, dim=0)\n",
    "                    # h_init = gen_init(ini_height, ini_discharge, downsampling=True)\n",
    "                    # data_condition = [h_gt, qx_gt, qy_gt]\n",
    "                    # data_condition0 = data_condition\\\n",
    "                    z = process_dem(dem_map)\n",
    "                    z = torch.unsqueeze(z, dim=0)\n",
    "                    input_data_list.append(input_data)\n",
    "                    h_gt_list.append(h_gt)\n",
    "                    u_gt_list.append(u_gt)\n",
    "                    v_gt_list.append(v_gt)\n",
    "                    z_list.append(z)\n",
    "            data_input = torch.cat(input_data_list, dim=0)\n",
    "            gt_h = torch.cat(h_gt_list, dim=0)\n",
    "            gt_u = torch.cat(u_gt_list, dim=0)\n",
    "            gt_v = torch.cat(v_gt_list, dim=0)\n",
    "            data_z = torch.cat(z_list, dim=0)\n",
    "            self.data_input = data_input\n",
    "            self.gt_h = gt_h\n",
    "            self.gt_u = gt_u\n",
    "            self.gt_v = gt_v\n",
    "            self.data_z = data_z\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_input[idx], self.gt_h[idx], self.gt_u[idx], self.gt_v[idx], self.data_z[idx]\n",
    "\n",
    "    def __len__(self, ):\n",
    "        return self.data_input.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "def train():\n",
    "    # # model\n",
    "    model = FNN3d(modes1=args.modes1, modes2=args.modes2, modes3=args.modes3, fc_dim=args.fc_dim,\n",
    "                  layers=args.layers).to(device)\n",
    "    #model = nn.DataParallel(model,  device_ids=None, output_device=None, dim=0)\n",
    "    optimizer = Adam(model.parameters(), betas=(0.9, 0.999), lr=args.base_lr)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=args.milestones, gamma=args.scheduler_gamma)\n",
    "    # PATH = '/mnt/SSD2/qingsong/qinqsong/Berlin_flood/GeoPINS_FD_supervised_2/results_rand/pretrain/checkpoint_4500.pth'\n",
    "    # checkpoint = torch.load(PATH)\n",
    "    # model.load_state_dict(checkpoint['state_dict'])\n",
    "    # # print(checkpoint['state_dict'].keys())\n",
    "    # print('load model sucessfully epoch', checkpoint['epoch'])\n",
    "    # log_dir = '/mnt/SSD2/qingsong/qinqsong/data_Berlin2/Moabit/results_rand/'\n",
    "    # eval(model, log_dir)\n",
    "\n",
    "    model.train()\n",
    "    # input data\n",
    "    epochs = 500\n",
    "    # else:\n",
    "    #     epochs = 2500\n",
    "    pbar = range(epochs)\n",
    "    pbar = tqdm(pbar, dynamic_ncols=True, smoothing=0.05)\n",
    "    train_pino = 0.0\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    trainset = train_data(train=True)\n",
    "    train_loader = DataLoader(trainset, batch_size=1, shuffle=True, num_workers=0)\n",
    "    for e in range(epochs):\n",
    "        for i, data in enumerate(train_loader):\n",
    "            optimizer.zero_grad()    \n",
    "            input_data, gt_h, gt_u, gt_v, z = data\n",
    "            input_data, gt_h, gt_u, gt_v, z = input_data.to(device), gt_h.to(device), gt_u.to(device), gt_v.to(device), z.to(device)\n",
    "            #print(input_data.shape,gt_h.shape)\n",
    "            h_init = input_data[..., 0, -1]\n",
    "            init_condition = [h_init]\n",
    "            data_condition = [gt_h, gt_u, gt_v]\n",
    "            out = model(input_data)\n",
    "            # print(out.shape)\n",
    "            # boundary\n",
    "            output = out.permute(0, 3, 1, 2, 4)\n",
    "            outputH = output[:, :, :, :, 0].clone()\n",
    "            # torch.where(outputU > 0.0, outputU, 0.0)\n",
    "            outputU = output[:, :, :, :, 1].clone()\n",
    "            outputV = output[:, :, :, :, 2].clone()\n",
    "            # outputH = F.threshold(outputH, threshold=0, value=0)\n",
    "            # outputU = F.threshold(outputU, threshold=0, value=0)\n",
    "            # outputV = F.threshold(outputV, threshold=0, value=0)\n",
    "            loss_d, loss_c = GeoPC_loss(input_data, outputH, outputU, outputV, data_condition, init_condition)\n",
    "            total_loss = loss_c + loss_d\n",
    "            total_loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss = total_loss.item()\n",
    "            # train_pino += loss_f.item()\n",
    "            # train_loss += total_loss.item()\n",
    "            # if e % 50 == 0:\n",
    "            scheduler.step()\n",
    "            pbar.set_description(\n",
    "                (\n",
    "                    f'Epoch {e} '\n",
    "                    f'loss_d: {loss_d:.5f} '\n",
    "                    f'loss_c: {loss_c:.5f} '\n",
    "                )\n",
    "            )\n",
    "            # loss\n",
    "            tb.log_scalars(e, write_hparams=True,\n",
    "                           loss_d=loss_d)\n",
    "        if (e+1)%100 == 0:\n",
    "            log_dir = '/home/sirui/INNOMAUS/'#'/mnt/SSD2/qingsong/qinqsong/data_Berlin2/Moabit/results_rand/'\n",
    "            eval(model, log_dir)\n",
    "            # eval_high(model, dem_map, log_dir, t0, i, nt)\n",
    "            if args.save_model == True:\n",
    "                 state_dict = model.state_dict()\n",
    "                 torch.save({'epoch': e, 'state_dict': state_dict},\n",
    "                            log_dir + f'pretrain/checkpoint_%d.pth'%(e))\n",
    "            torch.cuda.empty_cache()\n",
    "    print('Done!')\n",
    "\n",
    "\n",
    "#eval\n",
    "def generate_movie_2D(key, test_x, test_y, preds_y, plot_title='', field=0, val_cbar_index=-1, err_cbar_index=-1,\n",
    "                      val_clim=None, err_clim=None, font_size=None, movie_dir='', movie_name='movie.gif',\n",
    "                      frame_basename='movie', frame_ext='jpg', remove_frames=True):\n",
    "    frame_files = []\n",
    "\n",
    "    if movie_dir:\n",
    "        os.makedirs(movie_dir, exist_ok=True)\n",
    "\n",
    "    if font_size is not None:\n",
    "        plt.rcParams.update({'font.size': font_size})\n",
    "\n",
    "    if len(preds_y.shape) == 4:\n",
    "        Nsamples, Nx, Ny, Nt = preds_y.shape\n",
    "        preds_y = preds_y.reshape(Nsamples, Nx, Ny, Nt, 1)\n",
    "        test_y = test_y.reshape(Nsamples, Nx, Ny, Nt, 1)\n",
    "    Nsamples, Nx, Ny, Nt, Nfields = preds_y.shape\n",
    "    print('preds_y', preds_y.shape)\n",
    "\n",
    "    pred = preds_y[key, ..., field]\n",
    "    true = test_y[key, ..., field]\n",
    "    error = torch.abs(pred - true)\n",
    "\n",
    "    a = test_x[key]\n",
    "    x = torch.linspace(0, 1, Nx + 1)[:-1]\n",
    "    y = torch.linspace(0, 1, Ny + 1)[:-1]\n",
    "    X, Y = torch.meshgrid(x, y)\n",
    "    t = a[0, 0, :, 2]\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    ax1 = axs[0]\n",
    "    ax2 = axs[1]\n",
    "    ax3 = axs[2]\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, 256))\n",
    "    colors[0] = [1, 1, 1, 1]\n",
    "    cmap = ListedColormap(colors)\n",
    "\n",
    "    pcm1 = ax1.pcolormesh(X, Y, true[..., val_cbar_index], cmap=cmap, label='true', shading='gouraud')\n",
    "    pcm2 = ax2.pcolormesh(X, Y, pred[..., val_cbar_index], cmap=cmap, label='pred', shading='gouraud')\n",
    "    pcm3 = ax3.pcolormesh(X, Y, error[..., err_cbar_index], cmap=cmap, label='error', shading='gouraud')\n",
    "\n",
    "    if val_clim is None:\n",
    "        val_clim = pcm1.get_clim()\n",
    "    if err_clim is None:\n",
    "        err_clim = pcm3.get_clim()\n",
    "\n",
    "    pcm1.set_clim(val_clim)\n",
    "    plt.colorbar(pcm1, ax=ax1)\n",
    "    ax1.axis('square')\n",
    "\n",
    "    pcm2.set_clim(val_clim)\n",
    "    plt.colorbar(pcm2, ax=ax2)\n",
    "    ax2.axis('square')\n",
    "\n",
    "    pcm3.set_clim(err_clim)\n",
    "    plt.colorbar(pcm3, ax=ax3)\n",
    "    ax3.axis('square')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    for i in range(Nt):\n",
    "        # Exact\n",
    "        ax1.clear()\n",
    "        pcm1 = ax1.pcolormesh(X, Y, true[..., i], cmap=cmap, label='true', shading='gouraud')\n",
    "        pcm1.set_clim(val_clim)\n",
    "        ax1.set_title(f'Hydraulic Model {plot_title}: Maximum')\n",
    "        ax1.axis('square')\n",
    "\n",
    "        # Predictions\n",
    "        ax2.clear()\n",
    "        pcm2 = ax2.pcolormesh(X, Y, pred[..., i], cmap=cmap, label='pred', shading='gouraud')\n",
    "        pcm2.set_clim(val_clim)\n",
    "        ax2.set_title(f'KI-Tool {plot_title}: Maximum')\n",
    "        ax2.axis('square')\n",
    "\n",
    "        # Error\n",
    "        ax3.clear()\n",
    "        pcm3 = ax3.pcolormesh(X, Y, error[..., i], cmap=cmap, label='error', shading='gouraud')\n",
    "        pcm3.set_clim(err_clim)\n",
    "        ax3.set_title(f'Error {plot_title}: Maximum')\n",
    "        ax3.axis('square')\n",
    "\n",
    "        #         plt.tight_layout()\n",
    "        fig.canvas.draw()\n",
    "\n",
    "        if movie_dir:\n",
    "            frame_path = os.path.join(movie_dir, f'{frame_basename}-{i:03}.{frame_ext}')\n",
    "            frame_files.append(frame_path)\n",
    "            plt.savefig(frame_path)\n",
    "\n",
    "    if movie_dir:\n",
    "        movie_path = os.path.join(movie_dir, movie_name)\n",
    "        with imageio.get_writer(movie_path, mode='I') as writer:\n",
    "            for frame in frame_files:\n",
    "                image = imageio.imread(frame)\n",
    "                writer.append_data(image)\n",
    "\n",
    "    # if movie_dir and remove_frames:\n",
    "    #     for frame in frame_files:\n",
    "    #         try:\n",
    "    #             os.remove(frame)\n",
    "    #         except:\n",
    "    #             pass\n",
    "\n",
    "def eval(model, log_dir):\n",
    "    model.eval()\n",
    "    avg_err_hr = []\n",
    "    avg_err_ha = []\n",
    "    avg_err_ur = []\n",
    "    avg_err_ua = []\n",
    "    avg_err_vr = []\n",
    "    avg_err_va = []\n",
    "    t0 = 25\n",
    "    valset = train_data(train=False)\n",
    "    val_loader = DataLoader(valset, batch_size=1, shuffle=False, num_workers=0)\n",
    "    # lmbdleft, lmbdright = 0, (dem_map.shape[0] - 1)\n",
    "    # thtlower, thtupper = 0, (dem_map.shape[1] - 1)\n",
    "    # m = dem_map.shape[0]\n",
    "    # n = dem_map.shape[1]\n",
    "    # x = np.linspace(lmbdleft, lmbdright, m)\n",
    "    # y = np.linspace(thtlower, thtupper, n)\n",
    "    # X, Y = torch.meshgrid(x, y, indexing='ij')\n",
    "    key = 0\n",
    "    for i, data in enumerate(val_loader):\n",
    "        input_data, gt_h, gt_u, gt_v, z = data\n",
    "        input_data, gt_h, gt_u, gt_v, z = input_data.to(device), gt_h.to(device), gt_u.to(device), gt_v.to(device), z.to(device)\n",
    "        gt_hm, gt_um, gt_vm = gt_h.permute(0, 2, 3, 1), gt_u.permute(0, 2, 3, 1), gt_v.permute(0, 2, 3, 1)\n",
    "        gt_hm, gt_um, gt_vm = torch.unsqueeze(gt_hm, dim=-1), torch.unsqueeze(gt_um, dim=-1), torch.unsqueeze(gt_vm, dim=-1)\n",
    "        gt_m = torch.cat((gt_hm, gt_um, gt_vm), dim=-1)\n",
    "        gt_m, _ = torch.max(gt_m, dim=3, keepdim=True)\n",
    "        gt_m = torch.rot90(gt_m, k=-1, dims=[1,2])\n",
    "        print('gt_m', gt_m.shape)\n",
    "        with torch.no_grad():\n",
    "            out = model(input_data)\n",
    "            outm, _ = torch.max(out, dim=3, keepdim=True)\n",
    "            outm = torch.rot90(outm, k=-1, dims=[1,2])\n",
    "            outm = torch.where(gt_m>0, outm, gt_m)\n",
    "            print('outm', outm.shape)\n",
    "        #MOIVE\n",
    "        movie_dir = '/home/sirui/INNOMAUS/%s/'%(str(i))#'/mnt/SSD2/qingsong/qinqsong/data_Berlin2/Moabit/results_rand/movie/%s/'%(str(i))\n",
    "        os.makedirs(movie_dir, exist_ok=True)\n",
    "        print(movie_dir)\n",
    "        #H\n",
    "        movie_name = 'H.gif'\n",
    "        frame_basename = 'H_frame'\n",
    "        frame_ext = 'jpg'\n",
    "        plot_title = \"$H$\"\n",
    "        field = 0\n",
    "        val_cbar_index = -1\n",
    "        err_cbar_index = -1\n",
    "        font_size = 12\n",
    "        remove_frames = True\n",
    "        generate_movie_2D(key, input_data.cpu(), gt_m.cpu(), outm.cpu(),\n",
    "                          plot_title=plot_title,\n",
    "                          field=field,\n",
    "                          val_cbar_index=val_cbar_index,\n",
    "                          err_cbar_index=err_cbar_index,\n",
    "                          movie_dir=movie_dir,\n",
    "                          movie_name=movie_name,\n",
    "                          frame_basename=frame_basename,\n",
    "                          frame_ext=frame_ext,\n",
    "                          remove_frames=remove_frames,\n",
    "                          font_size=font_size)\n",
    "\n",
    "        # #U\n",
    "        movie_name = 'U.gif'\n",
    "        frame_basename = 'U_frame'\n",
    "        frame_ext = 'jpg'\n",
    "        plot_title = \"$U$\"\n",
    "        field = 1\n",
    "        val_cbar_index = -1\n",
    "        err_cbar_index = -1\n",
    "        font_size = 12\n",
    "        remove_frames = True\n",
    "        generate_movie_2D(key, input_data.cpu(), gt_m.cpu(), outm.cpu(),\n",
    "                          plot_title=plot_title,\n",
    "                          field=field,\n",
    "                          val_cbar_index=val_cbar_index,\n",
    "                          err_cbar_index=err_cbar_index,\n",
    "                          movie_dir=movie_dir,\n",
    "                          movie_name=movie_name,\n",
    "                          frame_basename=frame_basename,\n",
    "                          frame_ext=frame_ext,\n",
    "                          remove_frames=remove_frames,\n",
    "                          font_size=font_size)\n",
    "        #\n",
    "        # #V\n",
    "        movie_name = 'V.gif'\n",
    "        frame_basename = 'V_frame'\n",
    "        frame_ext = 'jpg'\n",
    "        plot_title = \"$V$\"\n",
    "        field = 2\n",
    "        val_cbar_index = -1\n",
    "        err_cbar_index = -1\n",
    "        font_size = 12\n",
    "        remove_frames = True\n",
    "\n",
    "        generate_movie_2D(key, input_data.cpu(), gt_m.cpu(), outm.cpu(),\n",
    "                          plot_title=plot_title,\n",
    "                          field=field,\n",
    "                          val_cbar_index=val_cbar_index,\n",
    "                          err_cbar_index=err_cbar_index,\n",
    "                          movie_dir=movie_dir,\n",
    "                          movie_name=movie_name,\n",
    "                          frame_basename=frame_basename,\n",
    "                          frame_ext=frame_ext,\n",
    "                          remove_frames=remove_frames,\n",
    "                          font_size=font_size)\n",
    "\n",
    "        # h, h05, qx, qy = F.threshold(h, threshold=0, value=0), F.threshold(h, threshold=0.05, value=0), F.threshold(qx, threshold=0, value=0), F.threshold(qy, threshold=0, value=0)\n",
    "        # h_gt, qx_gt, qy_gt = data(i)\n",
    "        # h, u, v = out[:, :, :, :, :1], out[:, :, :, :, 1:2], out[:, :, :, :, 2:3]\n",
    "        # h, u, v = torch.squeeze(h), torch.squeeze(u), torch.squeeze(v)\n",
    "        # h, u, v = h.permute(2, 0, 1), u.permute(2, 0, 1), v.permute(2, 0, 1)\n",
    "        # h_g = torch.squeeze(gt_h)\n",
    "        # u_g = torch.squeeze(gt_u)\n",
    "        # v_g = torch.squeeze(gt_v)\n",
    "        # h_p = h.detach().cpu().numpy()\n",
    "        # # h_p = h_p.reshape(-1, 1)\n",
    "        # u_p = u.detach().cpu().numpy()\n",
    "        # # u_p = u_p.reshape(-1, 1)\n",
    "        # v_p = v.detach().cpu().numpy()\n",
    "        # # v_p = v_p.reshape(-1, 1)\n",
    "        #\n",
    "        # h_gt = h_g.detach().cpu().numpy()\n",
    "        # print('h_gt', h_gt.shape)\n",
    "        # print('h_p', h_p.shape)\n",
    "        # # h_gt = h_gt.reshape(-1, 1)\n",
    "        # u_gt = u_g.detach().cpu().numpy()\n",
    "        # # u_gt = u_gt.reshape(-1, 1)\n",
    "        # v_gt = v_g.detach().cpu().numpy()\n",
    "        # # v_gt = v_gt.reshape(-1, 1)\n",
    "        # mask_h = np.where(h_gt != 0, 1, 0)\n",
    "        # mask_u = np.where(u_gt != 0, 1, 0)\n",
    "        # mask_v = np.where(v_gt != 0, 1, 0)\n",
    "        # h_p = mask_h * h_p\n",
    "        # u_p = mask_u * u_p\n",
    "        # v_p = mask_v * v_p\n",
    "        #\n",
    "        # # error_h_relative = np.linalg.norm(h_gt - h_p, 2) / np.linalg.norm(h_gt, 2)\n",
    "        # # error_u_relative = np.linalg.norm(u_gt - u_p, 2) / np.linalg.norm(u_gt, 2)\n",
    "        # # error_v_relative = np.linalg.norm(v_gt - v_p, 2) / np.linalg.norm(v_gt, 2)\n",
    "        # error_h_abs = np.mean(np.abs(h_gt - h_p))\n",
    "        # error_u_abs = np.mean(np.abs(u_gt - u_p))\n",
    "        # error_v_abs = np.mean(np.abs(v_gt - v_p))\n",
    "        # # avg_err_hr.append(error_h_relative)\n",
    "        # # avg_err_ha.append(error_h_abs)\n",
    "        # # avg_err_ur.append(error_u_relative)\n",
    "        # # avg_err_ua.append(error_u_abs)\n",
    "        # # avg_err_vr.append(error_v_relative)\n",
    "        # # avg_err_va.append(error_v_abs)\n",
    "        # #Max MAE\n",
    "        # error_hmax_abs = np.mean(np.abs(np.max(h_gt, axis=0) - np.max(h_p, axis=0)))\n",
    "        # error_umax_abs = np.mean(np.abs(np.max(u_gt, axis=0) - np.max(u_p, axis=0)))\n",
    "        # error_vmax_abs = np.mean(np.abs(np.max(v_gt, axis=0) - np.max(v_p, axis=0)))\n",
    "        # # MAE for every time step\n",
    "        # hh = np.abs(h_gt - h_p)\n",
    "        # error_hh = np.mean(hh, axis=1)\n",
    "        # error_h_everytime = np.mean(error_hh, axis=1)\n",
    "        #\n",
    "        # uu = np.abs(u_gt - u_p)\n",
    "        # error_uu = np.mean(uu, axis=1)\n",
    "        # error_u_everytime = np.mean(error_uu, axis=1)\n",
    "        #\n",
    "        # vv = np.abs(v_gt - v_p)\n",
    "        # error_vv = np.mean(vv, axis=1)\n",
    "        # error_v_everytime = np.mean(error_vv, axis=1)\n",
    "        #\n",
    "        # #MAE  bigger than 1 cm\n",
    "        # h_gt1 = h_gt\n",
    "        # h_p1 = h_p\n",
    "        # u_gt1 = u_gt\n",
    "        # u_p1 = u_p\n",
    "        # v_gt1 = v_gt\n",
    "        # v_p1 = v_p\n",
    "        # h_gt1[h_gt1 < 0.1] = 0\n",
    "        # h_p1[h_p1 < 0.1] = 0\n",
    "        # u_gt1[u_gt1 < 0.1] = 0\n",
    "        # u_p1[u_p1 < 0.1] = 0\n",
    "        # v_gt1[v_gt1 < 0.1] = 0\n",
    "        # v_p1[v_p1 < 0.1] = 0\n",
    "        # error_h_01 = np.mean(np.abs(h_gt1 - h_p1))\n",
    "        # error_u_01 = np.mean(np.abs(u_gt1 - u_p1))\n",
    "        # error_v_01 = np.mean(np.abs(v_gt1 - v_p1))\n",
    "        #\n",
    "        # ##MAE  bigger than 5 cm\n",
    "        # h_gt5 = h_gt\n",
    "        # h_p5 = h_p\n",
    "        # u_gt5 = u_gt\n",
    "        # u_p5 = u_p\n",
    "        # v_gt5 = v_gt\n",
    "        # v_p5 = v_p\n",
    "        # h_gt5[h_gt5 < 0.5] = 0\n",
    "        # h_p5[h_p5 < 0.5] = 0\n",
    "        # u_gt5[u_gt5 < 0.5] = 0\n",
    "        # u_p5[u_p5 < 0.5] = 0\n",
    "        # v_gt5[v_gt5 < 0.5] = 0\n",
    "        # v_p5[v_p5 < 0.5] = 0\n",
    "        # error_h_05 = np.mean(np.abs(h_gt5 - h_p5))\n",
    "        # error_u_05 = np.mean(np.abs(u_gt5 - u_p5))\n",
    "        # error_v_05 = np.mean(np.abs(v_gt5 - v_p5))\n",
    "        #\n",
    "        # print('error_h_abs',error_h_abs)\n",
    "        # print('error_u_abs', error_u_abs)\n",
    "        # print('error_v_abs', error_v_abs)\n",
    "        #\n",
    "        #\n",
    "        # print('error_hmax_abs', error_hmax_abs)\n",
    "        # print('error_umax_abs', error_umax_abs)\n",
    "        # print('error_vmax_abs', error_vmax_abs)\n",
    "        #\n",
    "        #\n",
    "        # print('error_h_everytime', error_h_everytime)\n",
    "        # print('error_u_everytime', error_u_everytime)\n",
    "        # print('error_v_everytime', error_v_everytime)\n",
    "        #\n",
    "        # print('error_h_01', error_h_01)\n",
    "        # print('error_u_01', error_u_01)\n",
    "        # print('error_v_01', error_v_01)\n",
    "        #\n",
    "        # print('error_h_05', error_h_05)\n",
    "        # print('error_u_05', error_u_05)\n",
    "        # print('error_v_05', error_v_05)\n",
    "\n",
    "        # val_loss_h = criterion(h, h_g)\n",
    "        # val_err_h.append(val_loss_h.item())\n",
    "        # rel_loss_h = criterion2(h, h_g)\n",
    "        # rel_err_h.append(rel_loss_h.item())\n",
    "        # val_loss_u = criterion(u, u_g)\n",
    "        # val_err_u.append(val_loss_u.item())\n",
    "        # rel_loss_u = criterion2(u, u_g)\n",
    "        # rel_err_u.append(rel_loss_u.item())\n",
    "        # val_loss_v = criterion(v, v_g)\n",
    "        # val_err_v.append(val_loss_v.item())\n",
    "        # rel_loss_v = criterion2(v, v_g)\n",
    "        # rel_err_v.append(rel_loss_v.item())\n",
    "        # N = len(val_err_h)\n",
    "        # avg_err_h = np.mean(val_err_h)\n",
    "        # std_err_h = np.std(val_err_h, ddof=1) / np.sqrt(N)\n",
    "        # avg_err_u = np.mean(val_err_u)\n",
    "        # std_err_u = np.std(val_err_u, ddof=1) / np.sqrt(N)\n",
    "        # avg_err_v = np.mean(val_err_v)\n",
    "        # std_err_v = np.std(val_err_v, ddof=1) / np.sqrt(N)\n",
    "    # file0 = \"/mnt/SSD1/qinqsong/Berlin_flood/GeoPINS_FD_supervised_2/results_rand/avg_err_hr.txt\"\n",
    "    # with open(file0, 'a', encoding='utf-8') as f0:\n",
    "    #     f0.writelines(str(avg_err_hr) + '\\n')\n",
    "    # file1 = \"/mnt/SSD1/qinqsong/Berlin_flood/GeoPINS_FD_supervised_2/results_rand/avg_err_ha.txt\"\n",
    "    # with open(file1, 'a', encoding='utf-8') as f1:\n",
    "    #     f1.writelines(str(avg_err_ha) + '\\n')\n",
    "    #\n",
    "    # file01 = \"/mnt/SSD1/qinqsong/Berlin_flood/GeoPINS_FD_supervised_2/results_rand/avg_err_ur.txt\"\n",
    "    # with open(file01, 'a', encoding='utf-8') as f2:\n",
    "    #     f2.writelines(str(avg_err_ur) + '\\n')\n",
    "    # file11 = \"/mnt/SSD1/qinqsong/Berlin_flood/GeoPINS_FD_supervised_2/results_rand/avg_err_ua.txt\"\n",
    "    # with open(file11, 'a', encoding='utf-8') as f3:\n",
    "    #     f3.writelines(str(avg_err_ua) + '\\n')\n",
    "    #\n",
    "    # file02 = \"/mnt/SSD1/qinqsong/Berlin_flood/GeoPINS_FD_supervised_2/results_rand/avg_err_vr.txt\"\n",
    "    # with open(file02, 'a', encoding='utf-8') as f4:\n",
    "    #     f4.writelines(str(avg_err_vr) + '\\n')\n",
    "    # file12 = \"/mnt/SSD1/qinqsong/Berlin_flood/GeoPINS_FD_supervised_2/results_rand/avg_err_va.txt\"\n",
    "    # with open(file12, 'a', encoding='utf-8') as f5:\n",
    "    #     f5.writelines(str(avg_err_va) + '\\n')\n",
    "        \n",
    "\n",
    "\n",
    "def eval_high(model, dem_map, log_dir, t0, i, nt):\n",
    "    dem_map = dem_map[::12, ::12]\n",
    "    print('dem_map_high', dem_map.shape)\n",
    "    model.eval()\n",
    "    days = 14\n",
    "    t00, tfinal = 0, days * 24 * 60 * 60\n",
    "    dt = 30\n",
    "    lmbdleft, lmbdright = 0, dem_map.shape[0] * 30\n",
    "    thtlower, thtupper = 0, dem_map.shape[1] * 30\n",
    "    m = dem_map.shape[0]\n",
    "    n = dem_map.shape[1]\n",
    "    l = int((days * 24 * 60 * 60) / dt)\n",
    "\n",
    "    ta = np.linspace(t00, tfinal, l)\n",
    "    x = np.linspace(lmbdleft, lmbdright, m)\n",
    "    y = np.linspace(thtlower, thtupper, n)\n",
    "    data_star = np.hstack((x.flatten(), y.flatten(), ta.flatten()))\n",
    "    lb = data_star.min(0)\n",
    "    ub = data_star.max(0)\n",
    "    #input_data\n",
    "    gridx = torch.from_numpy(x)\n",
    "    gridx = gridx.reshape(1, m, 1, 1, 1).repeat([1, 1, n, t0, 1])\n",
    "    gridy = torch.from_numpy(y)\n",
    "    gridy = gridy.reshape(1, 1, n, 1, 1).repeat([1, m, 1, t0, 1])\n",
    "    t = ta[i * (t0 - 1):(i + 1) * (t0 - 1) + 1]\n",
    "    gridt = torch.from_numpy(t)\n",
    "    gridt = gridt.reshape(1, 1, 1, t0, 1).repeat([1, m, n, 1, 1])\n",
    "    input_data = torch.cat((gridt, gridx, gridy), dim=-1)\n",
    "    input_data = 2.0 * (input_data - lb) / (ub - lb) - 1.0\n",
    "    input_data = input_data.float().to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(input_data)\n",
    "    h, qx, qy = out[:,:,:,:,:1], out[:,:,:,:,1:2], out[:,:,:,:,2:3]\n",
    "    h, h05, qx, qy = F.threshold(h, threshold=0, value=0), F.threshold(h, threshold=0.05, value=0), F.threshold(qx, threshold=0, value=0), F.threshold(qy, threshold=0, value=0)\n",
    "    h, h05, qx, qy = torch.squeeze(h), torch.squeeze(h05), torch.squeeze(qx), torch.squeeze(qy)\n",
    "    h, h05, qx, qy = h.permute(2,0,1), h05.permute(2,0,1), qx.permute(2,0,1), qy.permute(2,0,1)\n",
    "    h, h05, qx, qy = h.cpu().detach().numpy(), h05.cpu().detach().numpy(), qx.cpu().detach().numpy(), qy.cpu().detach().numpy()\n",
    "    plot_steps = [n for n in range(t0)]\n",
    "    for m in plot_steps:\n",
    "        time = i*(t0-1) + m\n",
    "        # save as Image\n",
    "        _EPSILON = 1e-6\n",
    "        h_a = h[m, :, :]\n",
    "        h_a_05 = h05[m, :, :]\n",
    "        qx_a = qx[m, :, :]\n",
    "        qy_a = qy[m, :, :]\n",
    "        q_a = (qx_a ** 2 + qy_a ** 2 + _EPSILON) ** 0.5\n",
    "        # save\n",
    "        h_a_m = np.mean(h_a)\n",
    "        q_a_m = np.mean(q_a)\n",
    "        file11 = \"/mnt/SSD2/qingsong/qinqsong/data_Berlin2/Moabit/results_rand/height_high.txt\"\n",
    "        with open(file11, 'a', encoding='utf-8') as f11:\n",
    "            f11.writelines(str(h_a_m) + '\\n')\n",
    "        file12 = \"/mnt/SSD2/qingsong/qinqsong/data_Berlin2/Moabit/results_rand/discharge_high.txt\"\n",
    "        with open(file12, 'a', encoding='utf-8') as f12:\n",
    "            f12.writelines(str(q_a_m) + '\\n')\n",
    "        h_aa = Image.fromarray(h_a)\n",
    "        h_aa_05 = Image.fromarray(h_a_05)\n",
    "        qx_aa = Image.fromarray(qx_a)\n",
    "        qy_aa = Image.fromarray(qy_a)\n",
    "        q_aa = Image.fromarray(q_a)\n",
    "        h_aa.save(os.path.join(log_dir, 'h_high/%s.tiff'%time))\n",
    "        h_aa_05.save(os.path.join(log_dir, 'h05_high/%s.tiff' % time))\n",
    "        qx_aa.save(os.path.join(log_dir, 'qx_high/%s.tiff' % time))\n",
    "        qy_aa.save(os.path.join(log_dir, 'qy_high/%s.tiff' % time))\n",
    "        q_aa.save(os.path.join(log_dir, 'q_high/%s.tiff' % time))\n",
    "\n",
    "import os\n",
    "if __name__ == '__main__':\n",
    "    os.makedirs('/home/sirui', exist_ok=True)\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7281a4ab-5654-4b4f-ae76-9bdaae791b61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
